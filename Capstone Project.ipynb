{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Capstone Project: ETL Pipeline for Fire Department Calls and Temperature Data\n",
    "\n",
    "## Project Summary\n",
    "The goal of this project: \n",
    "* Download San Francisco Fire department Calls-For-Service data set from Kaggle to local machine, \n",
    "* Create an ETL pipeline that extracts the data from .../input, processes data using Spark, and loads the data back into .../output as set of dimensional tables. <br>\n",
    "\n",
    "\n",
    "This will allow analytics team to find insights of the fire calls-for-service.e.g.\n",
    "* What were the common types of fire calls?<br> \n",
    "* Which months within the year 2019 saw for the highest number of fire calls? <br>\n",
    "* Which neighborhood in SF generated the most fire calls in 2019?<br>\n",
    "* What is the longest reaction time when a fire call is trigged till the service is available?<br>\n",
    "* What is the fastest reaction time when a fire call is trigged till the service is available?<br>\n",
    "* How to improve in order to reduce the fire incident?<br>\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data<br>\n",
    "* Step 2: Explore and Assess the Data<br>\n",
    "* Step 3: Define the Data Model<br>\n",
    "* Step 4: Run ETL to Model the Data<br>\n",
    "* Step 5: Complete Project Write Up<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import configparser\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "### Scope\n",
    "In this project, we will aggregate fire call data by CallType to form first dimension table. Next we will aggregate city temperature data by year and month to form the second dimension table. The two datasets will be joined on year and month to form the fact table. The final database is optimized to query on fire call events to determine if temperature affects the number of fire incidents. Spark will be used to process the data.\n",
    "\n",
    " \n",
    "### Describe and Gather Data\n",
    "**Data Source 1** <br>\n",
    "San Francisco Fire Calls-For-Service <br>\n",
    "https://www.kaggle.com/san-francisco/sf-fire-data-incidents-violations-and-more\n",
    "    \n",
    "**Content** <br>\n",
    "Fire Calls-For-Service includes all fire units responses to calls. Each record includes the call number, incident number, address, unit identifier, call type, and disposition. All relevant time intervals are also included.\n",
    "\n",
    "**Data Source 2**<br>\n",
    "\n",
    "https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data?select=GlobalLandTemperaturesByCity.csv\n",
    "The temperature data comes from Kaggle. It is provided in csv format. Some relevant attributes include:\n",
    "\n",
    "**Content** <br>\n",
    "AverageTemperature = average temperature\n",
    "City = city name\n",
    "Country = country name\n",
    "Latitude= latitude\n",
    "Longitude = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Read in fire calls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5319351"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the fire calls data \n",
    "input_path = \"input/fire-department-calls-for-service.csv\"\n",
    "fire_df =spark.read.csv(input_path, header=True, mode=\"DROPMALFORMED\")\n",
    "fire_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Call Number: string (nullable = true)\n",
      " |-- Unit ID: string (nullable = true)\n",
      " |-- Incident Number: string (nullable = true)\n",
      " |-- Call Type: string (nullable = true)\n",
      " |-- Call Date: string (nullable = true)\n",
      " |-- Watch Date: string (nullable = true)\n",
      " |-- Received DtTm: string (nullable = true)\n",
      " |-- Entry DtTm: string (nullable = true)\n",
      " |-- Dispatch DtTm: string (nullable = true)\n",
      " |-- Response DtTm: string (nullable = true)\n",
      " |-- On Scene DtTm: string (nullable = true)\n",
      " |-- Transport DtTm: string (nullable = true)\n",
      " |-- Hospital DtTm: string (nullable = true)\n",
      " |-- Call Final Disposition: string (nullable = true)\n",
      " |-- Available DtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode of Incident: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- Station Area: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- Original Priority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- Final Priority: string (nullable = true)\n",
      " |-- ALS Unit: string (nullable = true)\n",
      " |-- Call Type Group: string (nullable = true)\n",
      " |-- Number of Alarms: string (nullable = true)\n",
      " |-- Unit Type: string (nullable = true)\n",
      " |-- Unit sequence in call dispatch: string (nullable = true)\n",
      " |-- Fire Prevention District: string (nullable = true)\n",
      " |-- Supervisor District: string (nullable = true)\n",
      " |-- Neighborhooods - Analysis Boundaries: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Supervisor Districts: string (nullable = true)\n",
      " |-- Fire Prevention Districts: string (nullable = true)\n",
      " |-- Current Police Districts: string (nullable = true)\n",
      " |-- Neighborhoods - Analysis Boundaries: string (nullable = true)\n",
      " |-- Zip Codes: string (nullable = true)\n",
      " |-- Neighborhoods (old): string (nullable = true)\n",
      " |-- Police Districts: string (nullable = true)\n",
      " |-- Civic Center Harm Reduction Project Boundary: string (nullable = true)\n",
      " |-- HSOC Zones: string (nullable = true)\n",
      " |-- Central Market/Tenderloin Boundary Polygon - Updated: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fire_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Read in temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in temperature data\n",
    "temperature_input_path = \"input/GlobalLandTemperaturesByCity.csv\"\n",
    "df_t =spark.read.csv(temperature_input_path, header=True)\n",
    "df_t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|dt        |AverageTemperature|AverageTemperatureUncertainty|City |Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|6.068             |1.7369999999999999           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1743-12-01|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-01-01|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-02-01|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-03-01|null              |null                         |Århus|Denmark|57.05N  |10.33E   |\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_t.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Cleaning the data\n",
    "For the temperature data,  drop all entries where AverageTemperature is NaN, then add year and month as new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|dt        |AverageTemperature|AverageTemperatureUncertainty|City |Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|6.068             |1.7369999999999999           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-04-01|5.7879999999999985|3.6239999999999997           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-05-01|10.644            |1.2830000000000001           |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-06-01|14.050999999999998|1.347                        |Århus|Denmark|57.05N  |10.33E   |\n",
      "|1744-07-01|16.082            |1.396                        |Århus|Denmark|57.05N  |10.33E   |\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter out entries with NaN average temperature\n",
    "df_t=df_t.filter(df_t.AverageTemperature != 'NaN')\n",
    "df_t.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8235082"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8190783"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate locations\n",
    "df_t=df_t.dropDuplicates(['dt', 'City', 'Country'])\n",
    "df_t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add 2 new columns \"year\" and \"month\" to df_t\n",
    "df_t = df_t.withColumn(\"year\", F.year(df_t.dt)) \\\n",
    "                 .withColumn(\"month\", F.month(df_t.dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----------+-------+--------+---------+----+-----+\n",
      "|dt        |AverageTemperature|AverageTemperatureUncertainty|City       |Country|Latitude|Longitude|year|month|\n",
      "+----------+------------------+-----------------------------+-----------+-------+--------+---------+----+-----+\n",
      "|1743-11-01|7.03              |1.611                        |Bielefeld  |Germany|52.24N  |7.88E    |1743|11   |\n",
      "|1743-11-01|7.318             |1.764                        |Kaliningrad|Russia |55.45N  |19.84E   |1743|11   |\n",
      "+----------+------------------+-----------------------------+-----------+-------+--------+---------+----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_t.show(2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Query temperature info for city \"San Francisco\"\n",
    "df_t_san = (df_t.select('dt','year','month', 'Country', 'City', \"AverageTemperature\").where(df_t.City == \"San Francisco\")).sort('dt',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+-------------+-------------+------------------+\n",
      "|        dt|year|month|      Country|         City|AverageTemperature|\n",
      "+----------+----+-----+-------------+-------------+------------------+\n",
      "|2013-09-01|2013|    9|United States|San Francisco|            20.471|\n",
      "|2013-08-01|2013|    8|United States|San Francisco|19.730999999999998|\n",
      "|2013-07-01|2013|    7|United States|San Francisco|20.656999999999996|\n",
      "|2013-06-01|2013|    6|United States|San Francisco|            19.759|\n",
      "|2013-05-01|2013|    5|United States|San Francisco|            17.434|\n",
      "|2013-04-01|2013|    4|United States|San Francisco|15.995999999999999|\n",
      "|2013-03-01|2013|    3|United States|San Francisco|13.505999999999998|\n",
      "|2013-02-01|2013|    2|United States|San Francisco|            10.229|\n",
      "|2013-01-01|2013|    1|United States|San Francisco|              8.32|\n",
      "|2012-12-01|2012|   12|United States|San Francisco|              8.95|\n",
      "|2012-11-01|2012|   11|United States|San Francisco|            13.367|\n",
      "|2012-10-01|2012|   10|United States|San Francisco|17.294999999999998|\n",
      "|2012-09-01|2012|    9|United States|San Francisco|            19.045|\n",
      "|2012-08-01|2012|    8|United States|San Francisco|            20.531|\n",
      "|2012-07-01|2012|    7|United States|San Francisco|            19.632|\n",
      "|2012-06-01|2012|    6|United States|San Francisco|            18.482|\n",
      "|2012-05-01|2012|    5|United States|San Francisco|            16.555|\n",
      "|2012-04-01|2012|    4|United States|San Francisco|             14.15|\n",
      "|2012-03-01|2012|    3|United States|San Francisco|            11.503|\n",
      "|2012-02-01|2012|    2|United States|San Francisco|            11.116|\n",
      "|2012-01-01|2012|    1|United States|San Francisco|             9.993|\n",
      "|2011-12-01|2011|   12|United States|San Francisco|             9.118|\n",
      "|2011-11-01|2011|   11|United States|San Francisco|            11.478|\n",
      "|2011-10-01|2011|   10|United States|San Francisco|17.294999999999998|\n",
      "+----------+----+-----+-------------+-------------+------------------+\n",
      "only showing top 24 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_t_san.show(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.1 Conceptual Data Model\n",
    "The first dimension table will contain events from the fire department calls data. The columns below will be extracted from the fire calls dataframe:\n",
    "\n",
    ">Call Number<br>\n",
    ">City<br>\n",
    ">Call Type<br>\n",
    ">Call year<br>\n",
    ">call month<br>\n",
    ">numberofcalls<br>\n",
    ">Latitude<br>\n",
    ">Longitude <br>\n",
    "\n",
    "       \n",
    "    \n",
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    ">AverageTemperature <br>\n",
    ">City <br>\n",
    ">Country <br>\n",
    ">Latitude= latitude<br>\n",
    ">Longitude = longitude<br>\n",
    "\n",
    "The fact table will contain information from the fire calls data joined with the city temperature data on City:\n",
    "\n",
    ">Call Number<br>\n",
    ">City<br>\n",
    ">Call Type<br>\n",
    ">Call year<br>\n",
    ">call month<br>\n",
    ">numberofcalls<br>\n",
    ">Latitude<br>\n",
    ">Longitude <br>\n",
    ">AverageTemperature<br>\n",
    ">Country<br>\n",
    "\n",
    "\n",
    "The tables will be saved to Parquet files partitioned by city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps are described below:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
